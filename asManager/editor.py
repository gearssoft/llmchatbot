import os

from langchain_openai.chat_models import ChatOpenAI

from langchain.prompts import ChatPromptTemplate,MessagesPlaceholder
from langchain.schema.runnable import RunnablePassthrough,RunnableLambda
from langchain.memory import ConversationSummaryBufferMemory

import streamlit as st

# .env íŒŒì¼ ë¡œë“œ ë° í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
from dotenv import load_dotenv
load_dotenv()


def log_chain(x):
    print("log : ", x) # ì´ì „ ë‹¨ê³„ì—ì„œ ë„˜ì–´ì˜¨ ë°ì´í„°ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
    st.write(x)
    return x

rbLogChain = RunnableLambda(log_chain)


st.set_page_config(
    page_title="instruction Chat",
    page_icon="ğŸ“ƒ",
)

instruction__file_path = './temp/inst.txt'
#/temp/inst.txt dì´ì—†ìœ¼ë©´ ë§Œë“¤ê¸°
if not os.path.exists(instruction__file_path):
    with open(instruction__file_path, 'w') as f:
        f.write("ë‹¹ì‹ ì€ AS ì ‘ìˆ˜ê´€ë¦¬ìì…ë‹ˆë‹¤. ê³ ê°ì˜ ë¬¸ì˜ì— ëŒ€í•œ ë‹µë³€ì„ í•˜ì„¸ìš”.")

# _inst.txt íŒŒì¼ ë¡œë“œ
with open(instruction__file_path, 'r') as f:
    _instruction = f.read()

    with st.sidebar:
        instruction = st.text_area("Instructions",
        _instruction, height=200)
        
        if st.button("Save"):
            with open(instruction__file_path, 'w') as f:
                f.write(instruction)
            st.write("Saved")

llm = ChatOpenAI(
    model_name=os.getenv('OPENAI_MODEL_NAME'), 
    # max_new_tokens=32, 
    temperature=0.1,
    timeout=10
    )

if 'memory' not in st.session_state:
        st.session_state.memory = ConversationSummaryBufferMemory(
            llm=llm,
            max_token_limit=120,
            return_messages=True,
        )
def load_memory(args):
    
    history = st.session_state.memory.load_memory_variables({}).get("history", [])
    
    return {
        'question': args['question'], 
        'history': history
    }

        
def create_chain(instruction):

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                f"""
ë‹¤ìŒ "ëª…ë ¹"ì— ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”.
"ëª…ë ¹" : {instruction}\n\n
=====================================\n
ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
{st.session_state.memory.load_memory_variables({}).get("history", [])}\n\n
=====================================\n
""",
            ),
            # MessagesPlaceholder(variable_name="history"),
            ("human", "ì§ˆë¬¸ : {question}"),
        ]
    )
    

    chain = (
        RunnableLambda(load_memory)        
        | prompt | rbLogChain
        | llm
    )
    
    return chain

question = st.chat_input("ask anything...")

def _update_chat_history_text():
    history = st.session_state.memory.load_memory_variables({}).get("history", [])
    with st.sidebar:
        st.write(history)

if question :
    st.write(f"instruction : {instruction}")
    st.write(f"you : {question}")
    
    chain = create_chain(instruction)
    
    _answer = chain.invoke({"question": question})
    st.write(_answer.content)
    
    with st.spinner("ëŒ€í™” ë‚´ìš© ìš”ì•½ì¤‘..."):
        st.session_state.memory.save_context(
            {"input": question},
            {"output": _answer.content},
        )
        _update_chat_history_text()
    
    
    
